# ðŸš‚ðŸ¤–ðŸª„Conductor

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Conductor is a deployable interactive workbench (People & Agents) that locally orchestrates multiple Language Models (LMs) with Python code execution and test assertion capabilities. 

It leverages Git (or a basic line-by-line diff if Git is not available) to track code changes, providing Aider-style diffs for easy review and integration.

## Features

*   **Dual LLM Interaction:** Engage with two separate LLMs sequentially for more comprehensive results.
*   **Safe Code Execution:** Run Python code generated by the LLMs within a sandboxed environment with restricted operations.
*   **Automated Test Assertions:** Define test cases using `TEST-ASSERT` blocks, and Conductor will automatically run them against the executed code.
*   **Test-Driven Generation:** Generation stops after a configurable number of successful test passes, encouraging test-driven development.
*   **Aider-style Git Diffs:** Track code changes with Git diffs (or a basic line-by-line diff if not in a Git repository), making it easy to review and integrate code into your projects.
*   **Real-time Streaming:** Responses from the LLMs are streamed in real time to the user interface.
*   **Conversation History:** View the entire conversation history with the LLMs.
*   **Status and Error Display:** Clear status updates and error messages guide you through the interaction.
*   **Customizable System Message:** Tailor the behavior of the LLMs by modifying the system message.
*   **Configurable Model IDs:** Easily switch between different local LLMs by updating the model IDs.
*   **Gradio-powered UI:** A user-friendly web interface powered by Gradio.

## Prerequisites

*   **Python 3.7+:** Ensure you have Python 3.7 or a newer version installed.
*   **LM Studio (or similar):** A local LLM server like LM Studio is required. You can download it from [LM Studio's website](https://lmstudio.ai/).
*   **Git (optional):** For the best diff tracking experience, Git should be installed and the project should be within a Git repository. If Git is not available, a basic line-by-line diff will be used.

## Installation

1. **Clone the Repository:**

    ```bash
    git clone <repository_url>
    cd conductor
    ```

2. **Create and Activate a Virtual Environment:**

    ```bash
    python3 -m venv .venv
    source .venv/bin/activate  # On Linux/macOS
    ```

    ```bash
    python3 -m venv .venv
    .venv\Scripts\activate  # On Windows
    ```

3. **Install Dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

    This will install `gradio`, `openai`, and `GitPython`.

## Configuration

1. **LM Studio Setup:**
    *   Launch LM Studio.
    *   Download the models you want to use (e.g., `exaone-3.5-32b-instruct@q4_k_m` and `qwq-32b-preview`).
    *   Start the local server e.g. in LM Studio (default: `http://localhost:1234`).

2. **Model IDs:**
    *   Open `gradio-llm-interface.py` in a text editor.
    *   Locate the `LLMManager.__init__` method.
    *   Update the `self.model_a_id` and `self.model_b_id` variables with the correct model IDs from LM Studio.

    ```python
    self.model_a_id = "your-model-a-id"  # e.g., "exaone-3.5-32b-instruct@q4_k_m"
    self.model_b_id = "your-model-b-id"  # e.g., "qwq-32b-preview"
    ```

3. **System Message (Optional):**
    *   In `LLMManager.__init__`, you can customize the `self.system_message` to modify the behavior of the LLMs. This message sets the context for the conversation and defines the `RUN-CODE` and `TEST-ASSERT` block formats.

    ```python
    self.system_message = {
        "role": "system",
        "content": """You are an AI assistant with Python code execution capabilities.
    
    1. For code execution, use:
    RUN-CODE
    ```python
    your_code_here
    ```
    
    2. For tests, use:
    TEST-ASSERT
    ```python
    assert condition, "Test message"
    ```
    
    3. Important rules:
    - Each block must start with its marker on its own line
    - Code must be within triple backticks with 'python' specified
    - Tests have access to variables from code execution
    - Generation stops after 2 successful test passes
    
    ... (rest of the system message)
    """
    }
    ```

4. **Test Pass Count:**
    *   To change the number of successful test passes required to stop generation, modify `self.max_passed_tests` in `LLMManager.__init__`.

5. **Logging:**
    *   Logs are stored in the `logs/` directory.
    *   Adjust the logging level in `gradio-llm-interface.py` using:

    ```python
    logging.basicConfig(level=logging.DEBUG)  # For verbose logging
    # or
    logging.basicConfig(level=logging.INFO)   # For less verbose logging
    ```

## Usage

1. **Start the Interface:**

    ```bash
    python gradio-llm-interface.py
    ```

2. **Access the UI:**
    *   Open your web browser and go to `http://localhost:1337` (or the address indicated in the terminal).

3. **Interact with the LLMs:**
    *   Enter your prompt in the "Input Message" textbox.
    *   Click "Submit" to send the message to Model A.
    *   The conversation and results will appear in the "Conversation & Results" textbox.
    *   Code execution and test results will also be displayed.
    *   If the required number of tests pass, generation will stop. Otherwise, Model B will be engaged.
    *   Click "Stop Generation" to manually stop the generation process.
    *   Click "Clear Conversation" to start a new conversation.

4. **View Diffs:**
    *   After code execution, click the "Show Diffs" button to view the Aider-style Git diffs of the changes made to `.py` files in the project.
    *   Click "Clear Diffs" to clear the diff history.

## Example Workflow

1. **Prompt:**

    ```
    Write a Python function to calculate the Fibonacci sequence up to n terms. Include tests for n=0, n=1, n=5, and n=-1 (should raise an error).
    ```

2. **Model A's Response (may vary):**

    ```
    RUN-CODE
    ```python
    def fibonacci(n):
        if n < 0:
            raise ValueError("n must be non-negative")
        elif n <= 1:
            return n
        else:
            a, b = 0, 1
            for _ in range(2, n + 1):
                a, b = b, a + b
            return b
    ```

    TEST-ASSERT
    ```python
    assert fibonacci(0) == 0, "fibonacci(0) should be 0"
    assert fibonacci(1) == 1, "fibonacci(1) should be 1"
    assert fibonacci(5) == 5, "fibonacci(5) should be 5"
    try:
        fibonacci(-1)
        assert False, "fibonacci(-1) should raise ValueError"
    except ValueError:
        pass
    ```
    ```

3. **Execution and Testing:**
    *   Conductor will execute the `RUN-CODE` block.
    *   It will then run the `TEST-ASSERT` block.
    *   The results (output and test outcomes) will be displayed.
    *   If tests pass, the diff will be generated and stored.

4. **Show Diffs:**
    *   Clicking "Show Diffs" might reveal a diff similar to this (if the code created/modified a file called `fib.py`):

    ```diff
    --- a/fib.py
    +++ b/fib.py
    @@ -0,0 +1,14 @@
    +def fibonacci(n):
    +    if n < 0:
    +        raise ValueError("n must be non-negative")
    +    elif n <= 1:
    +        return n
    +    else:
    +        a, b = 0, 1
    +        for _ in range(2, n + 1):
    +            a, b = b, a + b
    +        return b
    +
    ```

## Troubleshooting

*   **Package Installation Errors:** If you encounter errors during package installation, ensure your virtual environment is activated and you have the necessary permissions to install packages.
*   **LM Studio Connection Issues:** Verify that LM Studio is running and the local server is started. Check the port number (default: 1234) and make sure it matches the `base_url` in `LLMManager.__init__`.
*   **Model Not Found:** Double-check that the model IDs you've configured in `LLMManager.__init__` are correct and that the models are loaded in LM Studio.
*   **Git Errors:** If you get errors related to Git, make sure Git is installed and that the project is inside a Git repository. If you don't want to use Git, the code will fall back to a basic line-by-line diff.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

Contributions are welcome! Please feel free to open issues or submit pull requests on the project's repository page.
