# ðŸš‚ðŸ¤–ðŸª„Conductor

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Conductor is a(n GU/AP) interface that (locally) orchestrates multiple (remote and/or local) Language Models

Python code is executed and tested against assertions; via configurable allow/block list of operations.

![](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExdGZxeno0ZHhzZnZudjd2dmwxNG5uNHVlMHY4a24zNGMxeHVqcmIxMSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/GPnHHTkd386URrEwYb/giphy.webp)


## Features

*   **Multi LM Interaction:** Engage with multiple LMs sequentially for mob-style programming.
*   **Safe Code Execution:** Run Python Code within a sandboxed environment via allowed/restricted operations.
*   **Automated Test Assertions:** Define test cases using `TEST-ASSERT` blocks; Conductor runs them automatically.
*   **Test-Driven Generation:** Generation stops after a configurable number of successful test passes, encouraging test-driven development.
*   **Customizable System Message:** Tailor the behavior of the LMs by modifying the system message.
*   **Configurable Model IDs:** Easily switch between different local LLMs by updating the model IDs.
*  **Configurable API/Model**: You can use an `.env` file in order to specify the API url and the model id to use.


## Prerequisites

*   **Python 3.7+:** Ensure you have Python 3.7 or a newer version installed.
*   **LM Studio (or similar):** A local LLM server like LM Studio is required. You can download it from [LM Studio's website](https://lmstudio.ai/).

## Installation

1. **Clone the Repository:**

    ```bash
    git clone https://github.com/rabbidave/conductor
    cd conductor
    ```

2. **Install Dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

    This will install `gradio`, and `openai`.

## Configuration

1.  **LM Studio Setup:**

    *   Launch LM Studio.
    *   Download and load the models you want to use (e.g., `exaone-3.5-32b-instruct@q4_k_m` and `qwq-32b-preview`).
    *   Start the local server in LM Studio (default: `http://localhost:1234`).

2.  **Model IDs:**
    * On the first use of the project, the system will automatically create an `.env` file with the default values. You can modify this file to change the local server url and model ids.
    *   Locate the `.env` file in the project root folder.
    *   Update the model ids `MODEL_A_ID` and `MODEL_B_ID` variables with the correct model IDs from LM Studio.

    ```
    MODEL_A_ID="your-model-a-id"  # e.g., "exaone-3.5-32b-instruct@q4_k_m"
    MODEL_B_ID="your-model-b-id"  # e.g., "qwq-32b-preview"
     LOCAL_API_URL="http://localhost:1234/v1/"
    ```
3.  **System Message (Optional):**

    *   In `LLMManager.__init__`, you can customize the `self.system_message` to modify the behavior of the LLMs. This message sets the context for the conversation and defines the `RUN-CODE` and `TEST-ASSERT` block formats.

    ```python
    self.system_message = {
        "role": "system",
        "content": """You are an AI assistant with Python code execution capabilities.

    1. For code execution, use:
    RUN-CODE
    ```python
    your_code_here
    ```

    2. For tests, use:
    TEST-ASSERT
    ```python
    assert condition, "Test message"
    ```

    3. Important rules:
    - Each block must start with its marker on its own line
    - Code must be within triple backticks with 'python' specified
    - Tests have access to variables from code execution
    - Generation stops after 2 successful test passes

        ... (rest of the system message)
        """
        }
        ```
    
4.  **Test Pass Count:**

    *   To change the number of successful test passes required to stop generation, modify `self.max_passed_tests` in `LLMManager.__init__` in the main python file.

5.  **Logging:**

    *   Logs are stored in the `logs/` directory.
    *   Adjust the logging level in `gradio-llm-interface.py` using:

    ```python
    logging.basicConfig(level=logging.DEBUG)  # For verbose logging
    # or
    logging.basicConfig(level=logging.INFO)   # For less verbose logging
    ```
    *   Debug level logging can be enabled to provide very detailed output on the behaviour of the code.

6. **Security Note**
 * Always use caution with code generated by an LLM and do not execute code from untrusted sources.

## Usage

1. **Start the Interface:**

    ```bash
    python app.py
    ```

2. **Access the UI:**

    *   Open your web browser and go to `http://localhost:31337` (or the address indicated in the terminal).

3. **Interact with the LMs:**

    *   Enter your prompt in the "Input Message" textbox.
    *   Click "Submit" to send the message to Model A.
    *   The conversation and results will appear in the "Conversation & Results" textbox.
    *   Code execution and test results will also be displayed.
    *   If the required number of tests pass, generation will stop. Otherwise, Model B will be engaged.
    *   Click "Stop Generation" to manually stop the generation process.
    *   Click "Clear Conversation" to start a new conversation.


## Example Workflow

1. **Prompt:**

    ```
    Write a Python function to calculate the nth term of the Fibonacci sequence using recursion. Include tests to validate the results for n=0, n=1, n=5, and n=10.
    ```

2. **Model A's Response (may vary):**

    ```
    RUN-CODE
    ```python
    def fibonacci_recursive(n):
        if n <= 0:
            return 0
        elif n == 1:
            return 1
        else:
            return fibonacci_recursive(n-1) + fibonacci_recursive(n-2)
    ```

    TEST-ASSERT
    ```python
    assert fibonacci_recursive(0) == 0, "fibonacci_recursive(0) should be 0"
    assert fibonacci_recursive(1) == 1, "fibonacci_recursive(1) should be 1"
    assert fibonacci_recursive(5) == 5, "fibonacci_recursive(5) should be 5"
    assert fibonacci_recursive(10) == 55, "fibonacci_recursive(10) should be 55"
    ```
    ```

3. **Execution and Testing:**

    *   Conductor will execute the `RUN-CODE` block.
    *   It will then run the `TEST-ASSERT` block.
    *   The results (output and test outcomes) will be displayed.
    *   The output in the UI will include the footer:

        ```
        Code block 1 output:

        ---
        Have fun y'all! ðŸ¤ ðŸª„ðŸ¤–
        ```

## Troubleshooting

*   **Package Installation Errors:** If you encounter errors during package installation, ensure your virtual environment is activated, and you have the necessary permissions to install packages.
*   **LM Studio Connection Issues:** Verify that LM Studio is running and the local server is started. Check the port number (default: 1234) and make sure it matches the `base_url` in `LLMManager.__init__` or in the `.env` file.
*   **Model Not Found:** Double-check that the model IDs you've configured in  the `.env` file are correct and that the models are loaded in LM Studio.
*   **Git Errors:** If you get errors related to Git, make sure Git is installed and that the project is inside a Git repository. If you don't want to use Git, the code will fall back to skipping diff generation.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

Contributions are welcome!  Please adhere to the following guidelines:

*   **Bug Reports:** Submit detailed bug reports including the steps to reproduce and any relevant error messages.
*   **Feature Requests:** Suggest new features, enhancements, or improvements in an issue.
*   **Pull Requests:** When submitting pull requests, make sure that your code aligns with the project's style and standards. Add tests where necessary and make sure all tests pass before submitting.

Please feel free to open issues or submit pull requests on the project's repository page.
